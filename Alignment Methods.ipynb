{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c66ca06",
   "metadata": {
    "cellId": "g89yvcufidrrmqmlkfkkrl",
    "execution_id": "85c42a96-a3d9-48da-903b-73b89766d863"
   },
   "source": [
    "### Установка библиотек, импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e2336",
   "metadata": {
    "cellId": "jmx3o65so2i2o8lhnji5h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "%pip uninstall keras tensorflow transformers\n",
    "%pip install --upgrade keras tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d9bcf721",
   "metadata": {
    "cellId": "2qckbeknhivaza2z17hjdi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:43:31.172082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-29 15:43:31.172300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-29 15:43:32.644269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-29 15:43:42.687873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, random_split\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, DPOTrainer\n",
    "from trl.core import LengthSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2a8bf54",
   "metadata": {
    "cellId": "8abtxsojnz5hdexe8q11f7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f583d0e",
   "metadata": {
    "cellId": "vbh46u4jkv9ckuq1w13",
    "execution_id": "3a173d3c-ef91-4b6b-9a04-a84e2e343166"
   },
   "source": [
    "### Настраиваем конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7b2b0f",
   "metadata": {
    "cellId": "nvoioidcinfy2pfjmjtg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=\"wandb\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "097a9440",
   "metadata": {
    "cellId": "q5vlm0xp6kpux0qgptskzq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "args = {\n",
    "    \"model_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"report_to\": \"wandb\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"max_length\": 512\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e9ae9",
   "metadata": {
    "cellId": "rx5ve1u7z7dtmx3qtiz3e",
    "execution_id": "4b8e2be1-7b06-44a9-b2f8-234f19ca9886"
   },
   "source": [
    "### Инициализация wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6ec82876",
   "metadata": {
    "cellId": "3dnlczqpg8ghjxexjswsph"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/timuruttsal/TinkoffLabAssignment/runs/mh598ede?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9da7f138e0>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "import wandb\n",
    "\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17136934",
   "metadata": {
    "cellId": "4gmazl95b955lpb5tu92is",
    "execution_id": "dd77d6ee-d5af-49ab-b03b-a586caf15a81"
   },
   "source": [
    "### Вспомогательная функция для генерации датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7e4b6233",
   "metadata": {
    "cellId": "ay2jjrjgl7ankcw3aobhfj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def build_dataset(args, dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args[\"model_name\"])\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "#     input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "    input_size = 8\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e85c1601",
   "metadata": {
    "cellId": "57dkrsxv7681i7f1oaz19e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dataset = build_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a114ff0b",
   "metadata": {
    "cellId": "id2gkso9l24efwtmocbxa"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dataset = dataset.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bb6ccd89",
   "metadata": {
    "cellId": "u72eq643symtstxylbsu0h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = AutoModelForCausalLM.from_pretrained(args[\"model_name\"])\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(args[\"model_name\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_name\"], padding_side='left')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d72c7b",
   "metadata": {
    "cellId": "0c6lzbtjuaadjxkqa82l12g",
    "execution_id": "ad723e1b-e918-4a46-80ac-ac1bad282ab6"
   },
   "source": [
    "### Посмотрим, как выглядят данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0b17f184",
   "metadata": {
    "cellId": "dsc9ai2o9nny1cjmqmcy1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   40, 26399,   314,  3001,   327, 47269, 20958,    12])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dataset[0][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1889f4a1",
   "metadata": {
    "cellId": "fl8t687ohnysfczep75vc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "tokenizer.decode(dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4d21067f",
   "metadata": {
    "cellId": "udqqkmzaby8s21onc4r2aj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dataset[0][\"review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2d21b",
   "metadata": {
    "cellId": "2a5b4bq4m6m3iv4nxras7k",
    "execution_id": "dbd4705e-5fde-4fe0-ad39-d13626041803"
   },
   "source": [
    "Мы видим, что в `input_ids` лежит обрезанное ревью. Пусть это будет нашим промптом. По этому промпту мы будем генерировать N выходов и среди них сделаем N-1 пар вида `winner-loser`, где `winner sentiment score` > `loser sentiment score`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04716f",
   "metadata": {
    "cellId": "3dq3zm1ynbk95y928e5fjr",
    "execution_id": "ae2caff6-1aa3-40b3-82ad-10580ac1fd64"
   },
   "source": [
    "### Генерация и оценка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1f29a569",
   "metadata": {
    "cellId": "9qukz884uhdjj2di9ffyf"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ee55838b",
   "metadata": {
    "cellId": "86a0ygi7g5vt5w6n8bjim"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d978a89b",
   "metadata": {
    "cellId": "oc065eds4fad2zehz5ji0q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 2.335048198699951},\n",
       " {'label': 'POSITIVE', 'score': -2.7265758514404297}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "text = \"this movie was really bad!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "00408c22",
   "metadata": {
    "cellId": "2u2tsezy5taf6t0b6i2tn5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 2.557040214538574},\n",
       " {'label': 'NEGATIVE', 'score': -2.294790267944336}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "text = \"this movie was really good!!\"\n",
    "sentiment_pipe(text, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7757471a",
   "metadata": {
    "cellId": "y92q6amfz4k0xcr4ulfbns"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "gen_kwargs = {\"min_length\": -1,\n",
    "              \"max_new_tokens\": 20,\n",
    "              \"top_k\": 0.0,\n",
    "              \"top_p\": 1.0,\n",
    "              \"do_sample\": True,\n",
    "              \"num_return_sequences\": 2,\n",
    "              \"pad_token_id\": tokenizer.eos_token_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "89a6bf56",
   "metadata": {
    "cellId": "vo2vwu8n8v6xakq0ac1k6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "generator = pipeline('text-generation', model=args[\"model_name\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d5ace0ac",
   "metadata": {
    "cellId": "dotf6nxjwfnskbbi2ov1f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOUS-AVIAN AND set a date for the tour with my favorite talent...and it never happened. D\n",
      "[{'label': 'NEGATIVE', 'score': 0.7753939628601074}, {'label': 'POSITIVE', 'score': -1.1945312023162842}]\n",
      "I rented I AM CURIOUS-BANDTHIS is basically just...all comedy in one campy poor-ass tale. Walk me\n",
      "[{'label': 'NEGATIVE', 'score': 1.9472341537475586}, {'label': 'POSITIVE', 'score': -2.3995108604431152}]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "generated_samples = generator(tokenizer.decode(dataset[0][\"input_ids\"]), **gen_kwargs)\n",
    "for sample in generated_samples:\n",
    "    output = sample[\"generated_text\"]\n",
    "    print(output)\n",
    "    print(sentiment_pipe(output, **sent_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415c2f5",
   "metadata": {
    "cellId": "ycdsv2c2x4a6tdgqbzs9li",
    "execution_id": "c7385c7d-b3b7-4ebf-b644-b9aa340afaf4"
   },
   "source": [
    "Мы научились генерировать и оценивать positive/negative label текстов. Настала пора создавать датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9ac8ba45",
   "metadata": {
    "cellId": "yptqsjx4jzc6b5cqm5rq5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   40, 26399,   314,  3001,   327, 47269, 20958,    12]),\n",
       " 'query': 'I rented I AM CURIOUS-'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3994895f",
   "metadata": {
    "cellId": "saa2li576jvh0tyd60lfn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 2.0228464603424072},\n",
       "  {'label': 'POSITIVE', 'score': -2.4736461639404297}],\n",
       " [{'label': 'POSITIVE', 'score': 2.4222044944763184},\n",
       "  {'label': 'NEGATIVE', 'score': -2.1730639934539795}]]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "output = [sample[\"generated_text\"] for sample in generated_samples]\n",
    "sentiment_samples = sentiment_pipe(output, **sent_kwargs)\n",
    "# sentiments = [result for result in sentiment_samples if result[\"label\"] == \"POSITIVE\"]\n",
    "# sentiments\n",
    "sentiment_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "62025116",
   "metadata": {
    "cellId": "270kbm7o8dmh00sfgjdj0ckj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c085ad4d",
   "metadata": {
    "cellId": "cawi4jmpl2j3hu2xg4mnlh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dataset = dataset.remove_columns(\"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "49403db4",
   "metadata": {
    "cellId": "1ft05hfu0am311hamfg0ok"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=sent_kwargs[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "726aefd0",
   "metadata": {
    "cellId": "sjmr2vtak2z452knezze"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   40, 26399,   314,  3001,   327, 47269, 20958,    12])\n",
      "tensor([    1,    40,  1703, 44269,    25, 12550,     1,   318])\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "print(dataset[0][\"input_ids\"])\n",
    "print(dataset[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f591c3c7",
   "metadata": {
    "cellId": "i5b5hmvfhtd8wyvrqy1bz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.2557511627674103},\n",
       " {'label': 'NEGATIVE', 'score': -0.3383306860923767}]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "sentiment_pipe(tokenizer.decode(dataset[0][\"input_ids\"]), **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1d212c5c",
   "metadata": {
    "cellId": "gq7q0thjquushu0llqvufs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 1.3919830322265625},\n",
       " {'label': 'POSITIVE', 'score': -1.791073203086853}]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "sentiment_pipe(tokenizer.decode(dataset[2][\"input_ids\"]), **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0f2e185c",
   "metadata": {
    "cellId": "asx3qs8dvjigteej5jofhe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "prompt_chosen_rejected_list = {\"prompt\": [], \"chosen\": [], \"rejected\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "852268d9",
   "metadata": {
    "cellId": "ay3r36kqog9324smnyryn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   40, 26399,   314,  3001,   327, 47269, 20958,    12])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cb08846a",
   "metadata": {
    "cellId": "qswc45aaw5m1zjsc839jjo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"hmmmm 70's (yum!) bloodbudget production and a 46 year old undercarriage produces\"},\n",
       " {'generated_text': ' e LCR has never been heard before again. A film with just a distaste for the subject'}]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "generator('', **gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "01444268",
   "metadata": {
    "cellId": "cfm9c2a6qme4vbf74dcx9h"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "generator(, **gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "45cb3724",
   "metadata": {
    "cellId": "qonrgtgsm2l73yh9e56h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "new_prompt_chosen_rejected_list = {\"chosen\": [], \"rejected\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "03b1f92c",
   "metadata": {
    "cellId": "u72tl00rdykxnn77d2qy5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'POSITIVE', 'score': 0.08281411975622177},\n",
       "  {'label': 'NEGATIVE', 'score': -0.3155623972415924}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.5138266682624817},\n",
       "  {'label': 'POSITIVE', 'score': -0.6836692690849304}],\n",
       " [{'label': 'POSITIVE', 'score': 0.24299603700637817},\n",
       "  {'label': 'NEGATIVE', 'score': -0.316336452960968}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.7443016767501831},\n",
       "  {'label': 'POSITIVE', 'score': -1.1051040887832642}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.7930777668952942},\n",
       "  {'label': 'POSITIVE', 'score': -1.1245578527450562}]]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "24f8f9f9",
   "metadata": {
    "cellId": "o03yi8uecmqchgc1af5f"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ae412295",
   "metadata": {
    "cellId": "rxy77ea5wva0fpjtbimwsqi",
    "execution_id": "4cd9a45a-83c7-4136-b491-cf5b14fb7d78"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "query = ''\n",
    "gen_kwargs[\"num_return_sequences\"] = 5\n",
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    gen_len = output_length_sampler()\n",
    "    gen_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    texts_gen = generator(query, **gen_kwargs)\n",
    "    texts = [elem['generated_text'] for elem in texts_gen]\n",
    "    sentiment_samples = sentiment_pipe(texts, **sent_kwargs)\n",
    "    positive_scores = []\n",
    "    for sent in sentiment_samples:\n",
    "        for dict_label_score in sent:\n",
    "            if dict_label_score['label'] == 'POSITIVE':\n",
    "                positive_scores.append(dict_label_score['score'])\n",
    "    max_index = positive_scores.index(max(positive_scores))\n",
    "    min_index = positive_scores.index(min(positive_scores))\n",
    "    new_prompt_chosen_rejected_list[\"chosen\"].append(texts[max_index])\n",
    "    new_prompt_chosen_rejected_list[\"rejected\"].append(texts[min_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6b7d534f",
   "metadata": {
    "cellId": "u5lcpihwvl8phxl507fw9"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "new_prompt_chosen_rejected_list[\"prompt\"] = [tokenizer.pad_token for _ in range(len(new_prompt_chosen_rejected_list[\"chosen\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "27a9ba69",
   "metadata": {
    "cellId": "9voboild15r2mu1yu42a3j"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "args = {\n",
    "    \"model_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"report_to\": \"wandb\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"max_length\": 512,\n",
    "    \"max_steps\": 15000,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"beta\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "356aacb3",
   "metadata": {
    "cellId": "f0k0dp29r9liy9okzscc68"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "78b6a7c4",
   "metadata": {
    "cellId": "msp7g34sr8nbojxmr3paa"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_split = {\"prompt\": new_prompt_chosen_rejected_list[\"prompt\"][:7000],\n",
    "                \"chosen\": new_prompt_chosen_rejected_list[\"chosen\"][:7000],\n",
    "                \"rejected\": new_prompt_chosen_rejected_list[\"rejected\"][:7000]}\n",
    "\n",
    "eval_split = {\"prompt\": new_prompt_chosen_rejected_list[\"prompt\"][7000:],\n",
    "                \"chosen\": new_prompt_chosen_rejected_list[\"chosen\"][7000:],\n",
    "                \"rejected\": new_prompt_chosen_rejected_list[\"rejected\"][7000:]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "080b0e4c",
   "metadata": {
    "cellId": "02qvk3ep2415p9rmwhuamjg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import datasets\n",
    "train_dataset = datasets.Dataset.from_dict(train_split)\n",
    "eval_dataset = datasets.Dataset.from_dict(eval_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "da4c82c7",
   "metadata": {
    "cellId": "nbznxdxpxrdo8hg43z9g7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=args[\"per_device_train_batch_size\"],\n",
    "        max_steps=args[\"max_steps\"],\n",
    "        remove_unused_columns=False,\n",
    "        gradient_accumulation_steps=args[\"gradient_accumulation_steps\"],\n",
    "        learning_rate=args[\"learning_rate\"],\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_first_step=True,\n",
    "        logging_steps=10,\n",
    "        eval_steps=4000,\n",
    "        output_dir=\"./test\",\n",
    "        optim=\"rmsprop\",\n",
    "        warmup_steps=100,\n",
    "        report_to=args[\"report_to\"],\n",
    "        gradient_checkpointing=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1c3a4a28",
   "metadata": {
    "cellId": "q0i1yc6aflr2kcaqqgf7eq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dpo_trainer = DPOTrainer(\n",
    "        model,\n",
    "        ref_model,\n",
    "        args=training_args,\n",
    "        beta=args[\"beta\"],\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=args[\"max_length\"],\n",
    "#         max_target_length=script_args.max_target_length,\n",
    "#         max_prompt_length=script_args.max_prompt_length,\n",
    "        generate_during_eval=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fe4d5",
   "metadata": {
    "cellId": "4ip5g67x3g4kn2mr79esu",
    "execution_id": "941a73af-8220-431f-992f-356b16ca0653"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2332' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2332/15000 05:11 < 28:11, 7.49 it/s, Epoch 5.32/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a9603",
   "metadata": {
    "cellId": "vvy0yv90jlls5nfsbjx6li"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(loader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    #### Get response from gpt2\n",
    "    response_tensors = []\n",
    "    for query in query_tensors:\n",
    "        gen_len = output_length_sampler()\n",
    "        gen_kwargs[\"max_new_tokens\"] = gen_len\n",
    "        response = generator(query, **gen_kwargs)\n",
    "        response = model.generate(query, **generation_kwargs)\n",
    "        response_tensors.append(response.squeeze()[-gen_len:])\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "    #### Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "    #### Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bd4ef491",
   "metadata": {
    "cellId": "4oegoejglg90tkklmjs27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 158/1556 [15:01<2:13:00,  5.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-86e7b7175c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgenerated_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_elem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerated_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     def preprocess(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 )\n\u001b[0;32m-> 1121\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1720\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2854\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0meos_token_id_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2855\u001b[0m                 unfinished_sequences = unfinished_sequences.mul(\n\u001b[0;32m-> 2856\u001b[0;31m                     \u001b[0mnext_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_token_id_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meos_token_id_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2857\u001b[0m                 )\n\u001b[1;32m   2858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for d in tqdm(loader):\n",
    "    query = d[\"query\"]\n",
    "    generated_samples = generator(query, **gen_kwargs)\n",
    "    texts = []\n",
    "    for batch_elem in generated_samples:\n",
    "        for x in batch_elem:\n",
    "            texts.append(x['generated_text'])\n",
    "            \n",
    "    sentiment_samples = sentiment_pipe(texts, **sent_kwargs)\n",
    "    positive_scores = []\n",
    "    for sent in sentiment_samples:\n",
    "        for dict_label_score in sent:\n",
    "            if dict_label_score['label'] == 'POSITIVE':\n",
    "                positive_scores.append(dict_label_score['score'])\n",
    "    # generate only 2 samples\n",
    "    prompt_chosen_rejected_list[\"prompt\"].extend(query)\n",
    "    for i in range(0, 2*len(query), 2):\n",
    "        if positive_scores[i] > positive_scores[i+1]:\n",
    "            prompt_chosen_rejected_list[\"chosen\"].append(texts[i])\n",
    "            prompt_chosen_rejected_list[\"rejected\"].append(texts[i+1])\n",
    "        else:\n",
    "            prompt_chosen_rejected_list[\"chosen\"].append(texts[i+1])\n",
    "            prompt_chosen_rejected_list[\"rejected\"].append(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d423a4",
   "metadata": {
    "cellId": "g8zoogme2nwbcumdgh8x"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4366818",
   "metadata": {
    "cellId": "wsjj3k1ziv4uiup1rkkpi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071d9f3",
   "metadata": {
    "cellId": "xpirvdmgx7f998ik9twsdq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dpo_trainer = DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29af31b6",
   "metadata": {
    "cellId": "7drkh7btls7mcjn96mju3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I rented I AM ERIVEN NEIGHBECK but I will check that another angel gets sent'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "response = ppo_trainer.generate(ppo_trainer.dataset[0][\"input_ids\"].to(device), **gen_kwargs)\n",
    "hr_response = tokenizer.decode(response.squeeze())\n",
    "hr_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8b718b5",
   "metadata": {
    "cellId": "k1tfe0qv04elwu1d5mrsia"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ppo_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fd128bd2c2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mppo_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ppo_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "ppo_trainer.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4a1d7",
   "metadata": {
    "cellId": "zvk54rkf3kb942a6mmcqi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "49f3972d-a3ff-487d-ba2d-30b3de1321f3",
  "notebookPath": "TinkoffLabAssignment/TinkoffLabAssignment/Alignment Methods.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
