# TinkoffLabAssignment

Файл куда смотреть: Alignment Methods.ipynb
Первоначальная идея заключалась в том, чтобы сгенерировать N текстов много раз и из N текстов выбрать `winner` как текст с максимальным скором, а `loser` с минимальным, соответственно.

Однако, обучив модель, я заметил, что средний positive score составляет `0.32`, что очень мало учитывая то, что первоначальные тестовые генерации показывали `score` $\approx$ 2-2.5, а после усреднения где-то 0.82 Более того, есть взглянуть на графики метрик, мы можем заменить, что мы повернули куда-то не туда: https://wandb.ai/timuruttsal/TinkoffLabAssignment/workspace?workspace=user-timuruttsal (График `Level1 with no prompts`)

Также я связываю низкое качество модели с тем, что первоначальные параметры генератора были подобраны неправильно: тексты получались слишком короткие, из-за чего вынести из них какой-то смысл модели было тяжело.

В связи с чем была предпринята попытка обучить модель, где сгенерированные пары `winner`-`loser` брались бы на основе промта из датасета `imdb`. Я взял промпты из `imdb dataset` (первые 8 токенов) и на них сгенерировал тексты большей длины, чем первоначально. Это всё еще не дотягивает до текста полноценного ревью, однако иначе я бы застрелился ждать миллион часов :)

После обучения модели (логи лежат в wandb, график `Level1 with prompts`) мы можем заметить, что на `eval dataset` средний reward модели увеличился, причём существенно. Также мы можем заметить, что `entropy` не поменялась в меньшую сторону -> `diversity` генерируемых токенов так же не уменьшилось.

![image](https://github.com/RickMortey/TinkoffLabAssignment/assets/47125236/817f0a6d-aab5-4279-850f-83e6362c6dc0)

![image](https://github.com/RickMortey/TinkoffLabAssignment/assets/47125236/02d28b70-5aa4-495c-9e6f-b5a26beced6b)

Это немного странно, потому что в DPO мы использовали KL дивергенцию, которая старается старается покрыть моду первоначальной SFT модели.
Мне кажется, она не сильно "сошлась" к моде ввиду малого количества эпох и малого значения $\beta$

Далее была обучена модель с `sigmoid loss`.

Логи обучения: в wandb. График `Level 1 with prompts (sigmoid loss)`

Результаты:

Средний positive score на eval dataset на стандартной модели: 0.8849
Средний positive score на новых снегерированных текстах на тех промтах на модели, обученной на генерацию более позитивных текстов: 2.0329
Прирост positive score: 1.1479

Энтропия базовой модели: 7.097806954517141
Энтропия модели, обученной с sigmoid loss: 7.379690160374308

Выводы по `Level 1`:

Давайте подведём итоги по моделям:

Средний positive score  `sft` модели: 0.8849

Средний positive score модели обученной с помощью `hinge loss`: 2.1767

Средний positive score модели обученной с помощью `sigmoid loss`: 2.0329


Энтропия `sft` модели: 7.097806954517141

Энтропия модели, обученной с `hinge loss`: 7.331948713070233

Энтропия модели, обученной с `sigmoid loss`: 7.379690160374308

С точки зрения награды (средний positive score), `sft`-модель имеет самый низкий показатель (0.8849), в то время как модели, обученные с помощью `hinge loss` и `sigmoid loss`, показывают значительно более высокие результаты соответственно (2.1767 и 2.0329). Это говорит о том, что тексты, генерируемые моделями, обученными с помощью `hinge loss` и `sigmoid loss`, получают в среднем более высокую награду.

В контексте вариативности ответов (энтропия), у `sft`-модели самый низкий показатель (7.0978), что указывает на меньшую степень разнообразия сгенерированных текстов по сравнению с остальными моделями. Модель, обученная с `sigmoid loss`, показывает самую высокую энтропию (7.3797) среди всех моделей, что указывает на большую вариативность в генерируемых текстах по сравнению с другими моделями.

В общем, кажется, что модели, обученные с помощью `hinge loss` и `sigmoid loss`, показывают более высокие результаты как в терминах средней награды, так и разнообразия генерируемых текстов, в то время как sft-модель дает более предсказуемые и менее награждаемые результаты.

Это немного странный результат, что мы получили бОльшую `diversity`, чем у первоначальной модели, т.к. по логике того, что KL-дивергенция в DPO работает на то, чтобы "покрыть" моду первоначального распределения, и на основе генерируемых текстов (а точнее того, что там есть сильный перекос в сторону слов с позитивной окраской), странно это видеть.

Ну а так, `Win-win`, получается


### Запуск
Для запуска ноутбука его нужно развернуть в среде с библиотеками, указанными в `requirements.txt`
Для этого можно использовать `venv`

Гайд:
# Переходим в директорию, где вы хотите разместить репозиторий,
$ cd <ваша выбранная директория>

# Клонируем репозиторий с github.com
$ git clone git@github.com:RickMortey/TinkoffLabAssignment.git

# Переходим в директорию склонированного репозитория
$ cd <мой репозиторий>

Версия Питона, на которой велась разработка: `3.10.12`

$ pyenv install 3.10.12

$ cd <путь к склонированному репозиторию>
$ ~/.pyenv/versions/3.10.12/bin/python -m venv venv

$ source env/bin/activate

# Установка зависимостей

(venv)$ pip install --upgrade -r requirements.txt

# Создание ядра в venv

(venv)$ ipython kernel install --user --name=venv

Убедитесь, что запуск ноутбука производится из под `env`

![image](https://github.com/RickMortey/TinkoffLabAssignment/assets/47125236/d887f5bc-8651-4411-9390-a58a0c1167e9)

# Удаление ядра

(venv)$ jupyter-kernelspec uninstall venv

# Выход из venv

(venv)$ deactivate
